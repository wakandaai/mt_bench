# mt_benchmark/config/models.yaml

# Toucan Models
toucan_base:
  model_name: "UBC-NLP/toucan-base"
  model_class: "MT5ForConditionalGeneration"
  torch_dtype: "float16"
  device_map: "auto"
  max_input_length: 1024
  generation_config:
    num_beams: 5
    do_sample: true
    temperature: 0.6
    top_p: 0.9
    max_new_tokens: 256

toucan_1.2B:
  model_name: "UBC-NLP/toucan-1.2B"
  model_class: "MT5ForConditionalGeneration"
  torch_dtype: "float16"
  device_map: "auto"
  max_input_length: 1024
  generation_config:
    num_beams: 5
    do_sample: true
    temperature: 0.6
    top_p: 0.9
    max_new_tokens: 256

# Seamless Models
seamless_m4t_v2:
  model_name: "facebook/seamless-m4t-v2-large"
  model_id: "seamless"
  model_class: "SeamlessM4Tv2Model"
  torch_dtype: "float16"
  device_map: "auto"
  max_input_length: 1024
  generation_config:
    num_beams: 5
    max_new_tokens: 256

# NLLB Models
nllb_200_distilled_600M:
  model_name: "facebook/nllb-200-distilled-600M"
  model_class: "NllbModel"
  torch_dtype: "float16"
  device_map: "auto"
  max_input_length: 1024
  generation_config:
    num_beams: 4
    max_new_tokens: 256

nllb_200_3.3B:
  model_name: "facebook/nllb-200-3.3B"
  model_class: "NllbModel"
  torch_dtype: "float16"
  device_map: "auto"
  max_input_length: 1024
  generation_config:
    num_beams: 4
    max_new_tokens: 256

# API Models (using DSPy unified interface)
gpt-5:
  model: "openai/gpt-5"
  rate_limit_delay: 0.05
  lm_kwargs:
    temperature: 1.0
    max_tokens: 20000

gpt-5-mini:
  model: "openai/gpt-5-mini"
  rate_limit_delay: 0.05
  lm_kwargs:
    temperature: 1.0
    max_tokens: 20000

gpt-5-nano:
  model: "openai/gpt-5-nano"
  rate_limit_delay: 0.0
  lm_kwargs:
    temperature: 1.0
    max_tokens: 20000

google-translate:
  type: "google_cloud_translate"
  credentials_path: "credentials.json"
  batch_size: 16